# Video-to-Notes Skill Usage Guide

Complete guide for using the video-to-notes skill to convert YouTube and Bilibili videos into AI-powered Markdown notes.

## Installation

### 1. Extract the Skill

If you have the `.skill` file, extract it to your desired location:

```bash
# Windows
powershell -Command "Expand-Archive -Path video-to-notes.skill -DestinationPath video-to-notes-skill"

# Linux/macOS
unzip video-to-notes.skill -d video-to-notes-skill
```

### 2. Install System Dependencies

#### FFmpeg (Required)

```bash
# Windows (using Chocolatey)
choco install ffmpeg

# macOS (using Homebrew)
brew install ffmpeg

# Linux (Ubuntu/Debian)
sudo apt-get update && sudo apt-get install ffmpeg

# Verify installation
ffmpeg -version
```

#### yt-dlp (Required)

```bash
# Using pip (recommended)
pip install yt-dlp

# Or using package managers
# Windows: choco install yt-dlp
# macOS: brew install yt-dlp

# Verify installation
yt-dlp --version
```

### 3. Install Python Dependencies

```bash
cd video-to-notes-skill/scripts
pip install -r requirements.txt
```

### 4. Configure API Keys

```bash
# Copy environment template
cp .env.example .env

# Edit .env and add your API keys
```

Edit `.env` file:

```env
# Required - Get from https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-proj-...

# Required - Get from https://openrouter.ai/
OPENROUTER_API_KEY=sk-or-...

# Optional configurations
AI_MODEL=anthropic/claude-3.5-sonnet
OUTPUT_DIRECTORY=./notes
TEMP_DIRECTORY=./temp
LOG_LEVEL=INFO
MAX_VIDEO_LENGTH=7200
```

## Basic Usage

### Process a YouTube Video

```bash
cd video-to-notes-skill/scripts
python process_video.py --url "https://www.youtube.com/watch?v=dQw4w9WgXcQ"
```

### Process a Bilibili Video

```bash
python process_video.py --url "https://www.bilibili.com/video/BV1xx411c7XZ" --language zh
```

### Save Output to File

```bash
python process_video.py \
  --url "https://www.youtube.com/watch?v=..." \
  --save-to-file \
  --output-path "./my_notes"
```

## Command Reference

### Syntax

```bash
python process_video.py \
  --url VIDEO_URL \
  [--language LANG] \
  [--ai-model MODEL] \
  [--save-to-file] \
  [--output-path PATH] \
  [--verbose]
```

### Arguments

| Argument | Required | Description | Example |
|----------|----------|-------------|---------|
| `--url` | Yes | Video URL (YouTube or Bilibili) | `--url "https://..."` |
| `--language` | No | Language code for transcription | `--language zh` |
| `--ai-model` | No | AI model for summarization | `--ai-model "google/gemini-2.5-flash"` |
| `--save-to-file` | No | Save output to file | `--save-to-file` |
| `--output-path` | No | Output directory | `--output-path "./notes"` |
| `--verbose` | No | Enable debug logging | `--verbose` |

## Examples

### Example 1: Basic YouTube Video

```bash
python process_video.py --url "https://www.youtube.com/watch?v=dQw4w9WgXcQ"
```

**Output** (stdout):
```markdown
# Never Gonna Give You Up - Rick Astley

## 核心要点
- Classic 1987 pop song performance
- Features Rick Astley's signature vocals
- Became internet meme phenomenon
...
```

### Example 2: Chinese Video with Language Hint

```bash
python process_video.py \
  --url "https://www.bilibili.com/video/BV1xx411c7XZ" \
  --language zh \
  --save-to-file
```

Creates: `./notes/视频标题.md`

### Example 3: Custom AI Model and Output Location

```bash
python process_video.py \
  --url "https://www.youtube.com/watch?v=..." \
  --ai-model "google/gemini-2.5-flash" \
  --save-to-file \
  --output-path "./学习笔记" \
  --verbose
```

### Example 4: Tutorial Video for Study Notes

```bash
python process_video.py \
  --url "https://www.youtube.com/watch?v=programming-tutorial" \
  --language en \
  --save-to-file \
  --output-path "./programming_notes"
```

## Output Format

The skill generates structured Markdown notes:

```markdown
# Video Title

## 核心要点
- Key takeaway 1
- Key takeaway 2
- Key takeaway 3
- Key takeaway 4
- Key takeaway 5

## 详细总结
A comprehensive summary of the video content, typically 200-300 words,
covering the main topics, arguments, and conclusions presented in the video.
The summary is generated by AI based on the full transcript.

---
**来源**: https://www.youtube.com/watch?v=...
**时长**: 0:08:45
**作者**: Channel Name
**处理时间**: 2025-01-15 14:30:00
```

## Use with Claude Skills

Once installed as a Claude skill, you can use natural language:

### Example Requests

**Simple request:**
```
Please process this video and create notes: https://www.youtube.com/watch?v=...
```

**With specifications:**
```
Convert this Bilibili video to markdown notes. Use Chinese for transcription:
https://www.bilibili.com/video/BV1xx411c7XZ

Save the output to my notes folder.
```

**Custom model:**
```
Process this video with Gemini model for faster results:
https://www.youtube.com/watch?v=...
```

## Performance & Costs

### Processing Time

For an **8-minute video**:
- Download: ~7 seconds
- Audio extraction: ~1 second
- Whisper transcription: ~10-15 seconds
- AI summarization: ~5-6 seconds
- **Total: ~25-30 seconds**

### API Costs

Per **8-minute video**:
- Whisper API: $0.006/min × 8 = **$0.048**
- OpenRouter (Claude 3.5 Sonnet): **~$0.02-0.05**
- **Total: ~$0.07-0.10**

For cost optimization, use cheaper models like `google/gemini-2.5-flash`.

## AI Model Options

### For Summarization (OpenRouter)

| Model | Quality | Speed | Cost | Use Case |
|-------|---------|-------|------|----------|
| `anthropic/claude-3.5-sonnet` | Excellent | Fast | Medium | Default, best balance |
| `google/gemini-2.5-flash` | Good | Very Fast | Low | Cost-effective, bulk processing |
| `openai/gpt-4-turbo` | Excellent | Medium | High | High-quality requirements |

Change model with `--ai-model`:
```bash
python process_video.py --url "..." --ai-model "google/gemini-2.5-flash"
```

## Limitations

### Video Length

- **Recommended**: <10 minutes (~25-30 second processing)
- **Maximum**: <20 minutes (Whisper API 25MB audio limit)
- **Configurable limit**: Adjustable via `MAX_VIDEO_LENGTH` in `.env`

### Supported Platforms

- ✅ YouTube
- ✅ Bilibili
- ❌ Other platforms (may work if supported by yt-dlp, but untested)

### Language Support

- Automatic language detection works for most languages
- Specify `--language` for better accuracy with Chinese, Japanese, Korean
- Supported languages: All languages supported by Whisper API

## Troubleshooting

### Quick Diagnostics

```bash
# Check FFmpeg
ffmpeg -version

# Check yt-dlp
yt-dlp --version

# Check Python packages
pip list | grep -E "yt-dlp|aiohttp|pydantic"

# Test with verbose mode
python process_video.py --url "..." --verbose
```

### Common Issues

#### "OpenAI API key required"
- Ensure `.env` exists in scripts directory
- Check `OPENAI_API_KEY=sk-...` is set correctly
- No spaces or quotes around the key

#### "FFmpeg failed"
- Verify FFmpeg is installed: `ffmpeg -version`
- Check PATH includes FFmpeg
- On Windows, restart terminal after installation

#### "Audio file too large: >25MB"
- Video is too long (>20-30 minutes)
- Use shorter video or compress audio manually
- Consider using local Whisper instead

#### "Video too long: XXXXs > 7200s"
- Video exceeds 2-hour default limit
- Adjust `MAX_VIDEO_LENGTH` in `.env`
- Or use shorter video

For detailed troubleshooting, see `references/troubleshooting.md`.

## Advanced Configuration

### Custom Models for Different Use Cases

**Fast bulk processing:**
```env
AI_MODEL=google/gemini-2.5-flash
```

**Highest quality summaries:**
```env
AI_MODEL=openai/gpt-4-turbo
```

**Language-specific models:**
```env
AI_MODEL=openai/gpt-4-turbo  # Better for English
AI_MODEL=anthropic/claude-3.5-sonnet  # Good multilingual
```

### Custom Output Directory

```env
OUTPUT_DIRECTORY=E:/学习笔记
TEMP_DIRECTORY=E:/临时文件
```

### Adjust Video Length Limits

```env
# Allow videos up to 4 hours
MAX_VIDEO_LENGTH=14400
```

### Enable Debug Logging

```env
LOG_LEVEL=DEBUG
```

## Tips & Best Practices

1. **Start with short videos** (<5 minutes) to verify setup
2. **Specify language** for non-English videos for better accuracy
3. **Use Gemini for bulk processing** to save costs
4. **Save important notes to files** with `--save-to-file`
5. **Check API usage** regularly on OpenAI and OpenRouter dashboards
6. **Keep temp directory clean** - script automatically cleans up
7. **Use verbose mode** when troubleshooting: `--verbose`

## Directory Structure

```
video-to-notes-skill/
├── SKILL.md                   # Skill definition for Claude
├── USAGE_GUIDE.md            # This file
├── README.md                 # Project overview
├── .env.example              # Environment template
├── scripts/                  # Core processing scripts
│   ├── process_video.py      # Main entry point
│   ├── core/                 # Core modules
│   │   ├── video_processor.py
│   │   ├── transcriber.py
│   │   ├── summarizer.py
│   │   └── exceptions.py
│   ├── config/
│   │   └── settings.py
│   └── requirements.txt
└── references/
    └── troubleshooting.md    # Detailed troubleshooting

Output directories (created automatically):
├── notes/                    # Saved markdown notes
└── temp/                     # Temporary processing files
```

## Next Steps

1. ✅ Install all dependencies
2. ✅ Configure API keys
3. ✅ Test with a short video
4. ✅ Save your first notes
5. ✅ Explore different models and options
6. ✅ Integrate with Claude for natural language usage

## Support & Resources

- **OpenAI Whisper API**: https://platform.openai.com/docs/guides/speech-to-text
- **OpenRouter**: https://openrouter.ai/docs
- **yt-dlp Documentation**: https://github.com/yt-dlp/yt-dlp
- **FFmpeg Documentation**: https://ffmpeg.org/documentation.html

For issues specific to this skill, consult `references/troubleshooting.md`.
